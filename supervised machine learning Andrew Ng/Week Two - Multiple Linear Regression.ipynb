{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb7d25a",
   "metadata": {},
   "source": [
    "# Week Two - Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e68c6e",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31828deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e9cbe",
   "metadata": {},
   "source": [
    "## Vector Creation\n",
    "Vectors are ordered arrays they can be of any data type but have to all be of the same type. In a maths setting a vector has a dimention of n thus runs from 1 to n, however in computing and python we will indexing from 0 to n - 1. Using this notation elements of a vectors are indexed using the vector the subscript postion of the element, for example: x<sub>i</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed11d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) : a = [0. 0. 0. 0.], a.shape = (4,), a data type = <class 'numpy.ndarray'>\n",
      "np.zeros(4) : a = [0. 0. 0. 0.], a.shape = (4,), a data type = <class 'numpy.ndarray'>\n",
      "np.zeros(4) : a = [0.50121588 0.18471461 0.7435729  0.38461816], a.shape = (4,), a data type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros(4); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {type(a)}\")\n",
    "a = np.zeros((4,)); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {type(a)}\")\n",
    "a = np.random.random_sample(4); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {type(a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3955fd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) : a = [0 1 2 3], a.shape = (4,), a data type = int32\n",
      "np.zeros(4) : a = [0.87800342 0.18121822 0.4728457  0.42489725], a.shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(4); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.random.rand(4); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a1be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.zeros(4) : a = [5 4 3 2], a.shape = (4,), a data type = int32\n",
      "np.zeros(4) : a = [5. 3. 3. 2.], a.shape = (4,), a data type = float64\n"
     ]
    }
   ],
   "source": [
    "a = np.array([5, 4, 3, 2]); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {a.dtype}\")\n",
    "a = np.array([5., 3, 3, 2]); print(f\"np.zeros(4) : a = {a}, a.shape = {a.shape}, a data type = {a.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c6caf",
   "metadata": {},
   "source": [
    "## Operations on Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b196fddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "a[2].shape = (). a[2] = 2. Accessing an element returns a scalar.\n",
      "a[-1] = 9\n",
      "index 10 is out of bounds for axis 0 with size 10\n"
     ]
    }
   ],
   "source": [
    "# Indexing.\n",
    "\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "# Access an element.\n",
    "print(f\"a[2].shape = {a[2].shape}. a[2] = {a[2]}. Accessing an element returns a scalar.\")\n",
    "\n",
    "# Accessing the last element in the array.\n",
    "print(f\"a[-1] = {a[-1]}\")\n",
    "\n",
    "# Indexes must be within the acceptable range of the vector or an error will be returned.\n",
    "try:\n",
    "    b = a[10]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fedbcba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "a[2:7:1] = [2 3 4 5 6]\n",
      "a[2:7:2] = [2 4 6]\n",
      "a[3:] = [3 4 5 6 7 8 9]\n",
      "a[:3] = [0 1 2]\n",
      "a[:] = [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Slicing.\n",
    "\n",
    "a = np.arange(10)\n",
    "print(a)\n",
    "\n",
    "# Accessing five conceutive elements (start:stop:step).\n",
    "c = a[2:7:1]\n",
    "print(f\"a[2:7:1] = {c}\")\n",
    "\n",
    "# Accessing 3 elements step 2 (start:stop:step).\n",
    "c = a[2:7:2]\n",
    "print(f\"a[2:7:2] = {c}\")\n",
    "\n",
    "# Access all elements index 3 and above.\n",
    "c = a[3:]\n",
    "print(f\"a[3:] = {c}\")\n",
    "\n",
    "# Access all elements below index 3 excluding 3.\n",
    "c = a[:3]\n",
    "print(f\"a[:3] = {c}\")\n",
    "\n",
    "\n",
    "# Access all elements.\n",
    "c = a[:]\n",
    "print(f\"a[:] = {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6cfcf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "[ 0 -1 -2 -3 -4]\n",
      "10\n",
      "2.0\n",
      "[ 0  1  4  9 16]\n"
     ]
    }
   ],
   "source": [
    "# Single Vector Operations.\n",
    "\n",
    "a = np.arange(5)\n",
    "print(a)\n",
    "\n",
    "# Negative elements of a.\n",
    "print(-a)\n",
    "\n",
    "# Summing all elements returns a scalar value.\n",
    "print(np.sum(a))\n",
    "\n",
    "# Finding the mean also returns a scalar value.\n",
    "print(np.mean(a))\n",
    "\n",
    "# Finding the square of every element.\n",
    "print(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8abfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary operators work element wise a+b: [0 0 6 8]\n",
      "operands could not be broadcast together with shapes (4,) (2,) \n"
     ]
    }
   ],
   "source": [
    "# Vector Vector Operations (Element-wise Operations)\n",
    "\n",
    "# Element-wise addition.\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, -2, 3, 4])\n",
    "print(f\"Binary operators work element wise a+b: {a+b}\")\n",
    "\n",
    "c = np.array([1, 2])\n",
    "try:\n",
    "    d = a + c\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7617f8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a * 2 = [ 0  2  4  6  8 10 12 14 16 18]\n"
     ]
    }
   ],
   "source": [
    "# Scalar Vector Operations - Vectors can be scaled by a number using *.\n",
    "a = np.arange(10)\n",
    "b = a * 2\n",
    "print(f\"a * 2 = {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5c37b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Vector Dot Product.\n",
    "# The dot product multiplies the values in two vectors element-wise and then sums the result.\n",
    "# Vector dot product requires the dimensions of the two vectors to be the same. \n",
    "\n",
    "# Using a for loop to perform vector vector dot product. Assuming a and b are the same dimention.\n",
    "\n",
    "def my_dot_product(a, b):\n",
    "    \"\"\"\n",
    "    Computes the dot product of two vectors of the same dimention.\n",
    "    Args:\n",
    "     - a - np array (n, ): representing our input vector.\n",
    "     - b - np array (n, ): representing our weights in linear regression.\n",
    "    Returns:\n",
    "     - x - Scalar value.\n",
    "    \"\"\"\n",
    "    x = 0\n",
    "    for i in range(a.shape[0]):\n",
    "        x += a[i] * b[i]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009079ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_dot_product(a, b) = 24\n"
     ]
    }
   ],
   "source": [
    "# Tesing For Loop Vector Vector Dot Product.\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "\n",
    "print(f\"my_dot_product(a, b) = {my_dot_product(a,b)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78858aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.dot(a, b) = 24\n"
     ]
    }
   ],
   "source": [
    "# Performing the same dot product but with np.dot.\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([-1, 4, 3, 2])\n",
    "c = np.dot(a, b)\n",
    "\n",
    "print(f\"np.dot(a, b) = {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83a92f",
   "metadata": {},
   "source": [
    "## Computation Speed: Loop Vs Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50658580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da765303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.(a, b) = 2501072.581681312\n",
      "Time taken to compute = 59.91840362548828\n",
      "my_dot_product(a, b) = 2501072.5816813707\n",
      "Time taken to compute = 2710.5798721313477\n"
     ]
    }
   ],
   "source": [
    "# Create two very large arrays.\n",
    "np.random.seed(1)\n",
    "a = np.random.rand(10000000)\n",
    "b = np.random.rand(10000000)\n",
    "\n",
    "# Capture time for np.dot\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"np.(a, b) = {c}\")\n",
    "print(f\"Time taken to compute = {1000 * (toc-tic)}\")\n",
    "\n",
    "# Capture time for my_dot_product\n",
    "tic = time.time()\n",
    "c = my_dot_product(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(f\"my_dot_product(a, b) = {c}\")\n",
    "print(f\"Time taken to compute = {1000 * (toc-tic)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6e92d",
   "metadata": {},
   "source": [
    "## Matrices\n",
    "It is common for input data (x_train) to come in the form of a vector of shape (m, ) of length m. Matrices are 2 dimentional arrays, index [m, n]. So a training set of data represented as a matrice is of length m and has n features. Thus w - our weights will come in the form of a 1 dimentional vector (n, ), representing one weight for each feature.\n",
    "\n",
    "The same functions used to create 1D vectors can be used to create 2D matrices. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c17b7d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape = (1, 5), a = [[0. 0. 0. 0. 0.]]\n",
      "a.shape = (2, 1), a = [[0.]\n",
      " [0.]]\n",
      "a.shape = (1, 1), a = [[0.44236513]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((1, 5))\n",
    "print(f\"a.shape = {a.shape}, a = {a}\")\n",
    "\n",
    "a = np.zeros((2, 1))\n",
    "print(f\"a.shape = {a.shape}, a = {a}\")\n",
    "\n",
    "a = np.random.random_sample((1, 1))\n",
    "print(f\"a.shape = {a.shape}, a = {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2880959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape = (3, 1), a = [[5]\n",
      " [4]\n",
      " [3]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# One can also specify data. Dimentions are determined by the use of additional brackets matching the required format.\n",
    "a = np.array([[5],[4],[3]])\n",
    "print(f\"a.shape = {a.shape}, a = {a}\")\n",
    "\n",
    "try: \n",
    "    print(a[0][0])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3dedd",
   "metadata": {},
   "source": [
    "## Operations on Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0778b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape = (3, 2) a = [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n",
      "\n",
      "a[2,0].shape =  () a[2,0] = 4 type(a[2,0]) = <class 'numpy.int32'>\n",
      "a[2].shape = (2,) a[2] = [4 5] type(a[2]) = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Indexing\n",
    "a = np.array([[0, 1], [2, 3], [4, 5]])\n",
    "print(f\"a.shape = {a.shape} a = {a}\")\n",
    "\n",
    "# Accessing an element.\n",
    "print(f\"\\na[2,0].shape =  {a[2, 0].shape} a[2,0] = {a[2, 0]} type(a[2,0]) = {type(a[2, 0])}\")\n",
    "\n",
    "# Accessing a row.\n",
    "print(f\"a[2].shape = {a[2].shape} a[2] = {a[2]} type(a[2]) = {type(a[2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13d11311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0 1 2 3 4 5 6 7 8 9] a.reshape(5, -1) = [[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping\n",
    "a = np.arange(10)\n",
    "print(f\"a = {a} a.reshape(5, -1) = {a.reshape(5, -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2055e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]]\n",
      "a[0, 2:7:1] = [2 3 4 5 6] a[0, 2:7:1].shape = (5,)\n",
      "a[:,:] = [[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]] a[:,:].shape = (2, 10)\n",
      "a[1,:] = [10 11 12 13 14 15 16 17 18 19]  a[1,:].shape = (10,)\n",
      "a[1] = [10 11 12 13 14 15 16 17 18 19] a[1].shape = (10,)\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "a = np.arange(20).reshape(-1, 10)\n",
    "print(f\"a = {a}\")\n",
    "\n",
    "# Access 5 consecutive elements (start:stop:step)\n",
    "print(f\"a[0, 2:7:1] = {a[0, 2:7:1]} a[0, 2:7:1].shape = {a[0, 2:7:1].shape}\")\n",
    "\n",
    "# access all elements\n",
    "print(f\"a[:,:] = {a[:,:]} a[:,:].shape = {a[:,:].shape}\")\n",
    "\n",
    "# access all elements in one row (very common usage)\n",
    "print(f\"a[1,:] = {a[1,:]}  a[1,:].shape = {a[1,:].shape}\")\n",
    "\n",
    "# The above is the same as.\n",
    "print(f\"a[1] = {a[1]} a[1].shape = {a[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c4a73",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "We are going to yet again be using housing prices as a motivating factor in our problem. This time the dataset contains three examples, and four different features, these are - Size, Bedrooms, Floors, Age. Then again the label we are trying to predict is the continuous price value/ dependent variable.\n",
    "\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6e51a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (3, 4)\n",
      "y_train.shape = (3,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "print(f\"X_train.shape = {X_train.shape}\")\n",
    "print(f\"y_train.shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acb49e",
   "metadata": {},
   "source": [
    "Since we are using multiple features this time w must be a vector representing one value for each feature in X_train, b however is still a scalar. We will select some inital values for w and b for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6da1768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_init.shape = (4,)\n",
      "type(b_init) = <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "print(f\"w_init.shape = {w_init.shape}\")\n",
    "print(f\"type(b_init) = {type(b_init)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af6a82",
   "metadata": {},
   "source": [
    "## Prediction Model with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f41af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_for_loop(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes linear regression model using multiple variables.\n",
    "    Arguments:\n",
    "     - X - The training features X_train, length m.\n",
    "     - w - The weight.\n",
    "     - b - The bias.\n",
    "    Returns:\n",
    "     - p - prediction\n",
    "    \"\"\"\n",
    "    m = x.shape[0]\n",
    "    p = 0\n",
    "    for i in range(m):\n",
    "        p_i = x[i] * w[i]\n",
    "        p = p + p_i\n",
    "    p += b\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8114fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row = [2104    5    1   45]\n",
      "459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# Get a row from the data.\n",
    "a = X_train[0, :]\n",
    "print(f\"First row = {a}\")\n",
    "\n",
    "# Get a prediction for this single row.\n",
    "print(lr_for_loop(a, w_init, b_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabc3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Prediction using np.dot. \n",
    "\n",
    "def np_lr(x, w, b):\n",
    "    p = np.dot(w, x) + b\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d396c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row = [2104    5    1   45]\n",
      "459.9999976194083\n"
     ]
    }
   ],
   "source": [
    "# Get a row from the data.\n",
    "a = X_train[0, :]\n",
    "print(f\"First row = {a}\")\n",
    "\n",
    "# Get a prediction for this single row.\n",
    "print(np_lr(a, w_init, b_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db04a64",
   "metadata": {},
   "source": [
    "## Computing the cost for multiple variables\n",
    "The cost function for multiple variables is exactly the same the only difference is how you calculate yhat, using vectors and matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f0244cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calculates the cost of the linear regression model.\n",
    "    Arguments:\n",
    "     - X - The training features X_train, length m, features n.\n",
    "     - w - The weights (m ,)\n",
    "     - b - The bias.\n",
    "     - y - The actual y values assoicated with our input values X.\n",
    "     Returns:\n",
    "     - total_cost - The toal (error) cost of our function for the values we have used for w and b. \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "            yHat = np.dot(X[i], w) + b\n",
    "            cost = cost + (yHat - y[i]) ** 2\n",
    "    total_cost = cost / (2 * m)\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15cae270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w  = 1.5578904045996674e-12\n"
     ]
    }
   ],
   "source": [
    "# Computing cost\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "cost = cost_function(X_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w  = {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4c8635",
   "metadata": {},
   "source": [
    "## Gradient with Multiple Variables\n",
    "To perform gradient descent with multiple variables I am going to use two for loops. The first for loop loops through all the examples then the second loop loops through all the features in that example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54315fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graident_function(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradeint for linear regression.\n",
    "    Arguments:\n",
    "     - X - The training features X_train, length m, features n.\n",
    "     - y - The actual y values assoicated with our input values X.\n",
    "     - w - The weights (m, )\n",
    "     - b - The bias.\n",
    "    Returns:\n",
    "     - gw - Gradient for paramater w.\n",
    "     - gb - Gradient for paramater b.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    gw = np.zeros((n,))\n",
    "    gb = 0\n",
    "    for i in range(m):\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            gw[j] = gw[j] + error * X[i, j]\n",
    "        gb = gb + error\n",
    "    gb = gb / m\n",
    "    gw = gw / m\n",
    "    return gw, gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e196d42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gb at initial w,b = -1.6739251122999121e-06\n",
      "gw at initial w,b =  [-2.72623574e-03 -6.27197255e-06 -2.21745574e-06 -6.92403377e-05]\n"
     ]
    }
   ],
   "source": [
    "# Compute and display gradient \n",
    "gw, gb = graident_function(X_train, y_train, w_init, b_init)\n",
    "print(f'gb at initial w,b = {gb}')\n",
    "print(f'gw at initial w,b =  {gw}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b97739",
   "metadata": {},
   "source": [
    "## Gradient Descent with Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42c3aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, alpha, num_iter, cost_function, graident_function):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent using the whole dataset in each iteration. Update w and b by taking num_iterations gradient \n",
    "    steps in the right direction size of learning rate alpha.\n",
    "    Arguments:\n",
    "     - X - The training features X_train, length m, features n.\n",
    "     - y - The weights for our model, length n. \n",
    "     - b - The bias (scalar).\n",
    "     - cost_function - The cost function returns the cost for the current values of w and b.\n",
    "     - gradient_function - Computes the gradient of the cost for the current values of w and b.\n",
    "     - alpha - The learning rate, 0.01 would be sensible.\n",
    "     - num_iterations - The number of times we want to perform gradient  descent over the whole dataset (batch gradient descent)\n",
    "    Returns:\n",
    "     - w_optimal - A vector of w values length n, at optimal.\n",
    "     - b_optimal - A scalar value for b, at optimal\n",
    "    \"\"\"\n",
    "    # An array to store the cost history (we want this to decrese, implying that our model is diverging).\n",
    "    cost_hist = []\n",
    "    w = np.zeros(X.shape[1])\n",
    "    b = b_init\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        # Calculate the gradient.\n",
    "        gw, gb = graident_function(X, y, w, b)\n",
    "        \n",
    "        w = w - alpha * gw \n",
    "        b = b - alpha * gb \n",
    "    \n",
    "        if i < 100000:\n",
    "            cost_hist.append(cost_function(X, y, w, b))\n",
    "            \n",
    "        # Print the cost 10 times.\n",
    "        if i % math.ceil(num_iter / 10 ) == 0:\n",
    "            print(f\"Iteration: {i}, cost: {cost_hist[-1]}\")\n",
    "    print(f\"Cost at optimal: {cost_hist[-1]} b = {w}, b = {b}.\")\n",
    "    \n",
    "    return w, b, cost_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "45bff311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, cost: 2529.4629522316304\n",
      "Iteration: 10000, cost: 624.822259314153\n",
      "Iteration: 20000, cost: 594.3333440477821\n",
      "Iteration: 30000, cost: 581.0058048286584\n",
      "Iteration: 40000, cost: 574.732192894754\n",
      "Iteration: 50000, cost: 571.3591014150833\n",
      "Iteration: 60000, cost: 569.1797876455938\n",
      "Iteration: 70000, cost: 567.4929040444463\n",
      "Iteration: 80000, cost: 566.0102450017911\n",
      "Iteration: 90000, cost: 564.613375224772\n",
      "Cost at optimal: 563.2537571994991 b = [ 0.24224154  0.28821169 -0.85520022 -1.57622854], b = -0.04168501764938912.\n"
     ]
    }
   ],
   "source": [
    "# Testing out gradient descent\n",
    "w_start = np.zeros(X_train[1])\n",
    "b_start = 0\n",
    "iterations = 100000\n",
    "alpha = 5.0e-7\n",
    "\n",
    "w, b, cost = gradient_descent(X_train, y_train, w_start, b_start, alpha, iterations, cost_function, graident_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb2b3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models prediction after being trained with batch gradient descent: 439.29 compaired to the actual value of 460\n"
     ]
    }
   ],
   "source": [
    "# Testing on the first example in training data, X_train[0] & y_train[0].\n",
    "\n",
    "X_train_sample = X_train[0]\n",
    "\n",
    "yHat_sample = np.dot(X_train_sample, w) + b\n",
    "print(f\"Models prediction after being trained with batch gradient descent: {yHat_sample:0.2f} compaired to the actual value of {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb7b9a",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "In this wee section we will look two different scaling techniques:\n",
    "- Mean Normalization:\n",
    "$$ Xnormalized = (X - μ) / (max(X) - min(X)) $$\n",
    "Where:\n",
    "- μ - is the mean of that value of that feature over the whole dataset.\n",
    "- max(X) - is the max value of that variable.\n",
    "- min(X) is the min value of that variable.\n",
    "\n",
    "- Z-Score Normalization:\n",
    "$$ X_normalized = (X - μ) / σ $$\n",
    "Where: \n",
    "- μ - The mean value of that feature over the whole dataset.\n",
    "- σ - The standard deviation of that feature\n",
    "\n",
    "After z-score normalization the feature should have a mean of 0 and a standard deviation of 1. The mean and standard deviation can be calculated as shown:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eae5cdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization(X):\n",
    "    \"\"\"\n",
    "    Computes z-score normalization column by column.\n",
    "    Arguments:\n",
    "     - X - (m, n) The training features X_train, length m, features n.\n",
    "    Returns:\n",
    "     - X_norm - (m, m)Input X normalized by column.\n",
    "     - mu - (n, )The mean of each column.\n",
    "     - sigma - (n, )The stadard deviation of each feature/ column.\n",
    "    \"\"\"\n",
    "    # Get the mean of each column.\n",
    "    mu = np.mean(X, axis = 0)\n",
    "    \n",
    "    # Get standard deviation of each column.\n",
    "    sigma = np.std(X, axis = 0)\n",
    "    \n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cc922",
   "metadata": {},
   "source": [
    "# :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
